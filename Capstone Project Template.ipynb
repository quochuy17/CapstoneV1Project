{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title : Capstone V1\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "##### The core purpose of the Capstone V1 project is just to help learners like us combine what we have learned throughout the program. This project will be an important part of our portfolio that will help us achieve the data engineering-related career goals.\n",
    "##### In this project, we can choose to complete the project provided , or define the scope and data for a project with the own design using some provided data sources like Kaggle, Google Data Search, Data Gov,... \n",
    "\n",
    "### Project Goal\n",
    "##### The core target of Capstone V1 project is to make a chance for those people or analytic groups such as data researchers, data analysts and other groups in order to analyze the large contexts of the US immigration data through gathering the data sources from four distinct datasets including:\n",
    "    .Immigration.\n",
    "    .Airport codes.\n",
    "    .US Cities demographics. \n",
    "    .Global temperature. \n",
    "    .Through above dataset resources, the project built with a star schema which will enable data researchers, data analysts and other groups to get the insights of specific dataset. For instance, which city or area has more tourists visiting the United States, which states attract the most immigrant people, how long do they visit the US, what's the demography of the state where the immigrants see as the destination, and what's the average temparature of the US or other nations and the most common types of weather corresponding to each of temperature.\n",
    "\n",
    "### Requirement: Working with four datasets to complete the project including:\n",
    "        . \"immigration_data_sample.csv\" : This is the main dataset which including data on immigration to the United States.\n",
    "        . \"airport-codes_csv.csv\" : The supplementary dataset will include data on airport codes of countries in some continents like NA, EU, AF, AS, OC.\n",
    "        . \"us-cities-demographics.csv\" : This is also the supplementary dataset U.S. city demographics which provides the information about the demographics in some states, cities.\n",
    "        . Lastly, the temperature data which describes the general information of temperature in some areas.  \n",
    "    - In addition, also welcoming to enrich the project with some sources of additional data in case to set the own project apart.\n",
    "\n",
    "### The \"Capstone V1 \" project consists of the following main steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# Import the Pyspark resources\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear, avg, monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, date_format\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData, HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_add as d_add\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Import other files for usage\n",
    "import main as main \n",
    "import project_configuration as config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "- Project's goal : discover the insights of these datasets in the level of high-detail and make a project with a high level of applying to the real world in some cases. Besides, create a database that can be used as a resource for international immigrants and the US governement. \n",
    "- This project results could help policy makers make informed decisions on immigration that the goverments can control carefully the immigrantion people and confirm thier status like name, age, health and other personal information. In addition, it also help these people obtain a chance to stay in the US by using the valuable insights of the data for permanent accommodation here.\n",
    "    \n",
    "#### Describe and Gather Data \n",
    "##### The data usage consisting of : 4 main datasets (mentioned partly on the project requirement above)\n",
    "##### Data Sources : \n",
    "    . \"I94 Immigration Data\": This data is from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in.\n",
    "    . \"World Temperature Data\": This dataset came from Kaggle. \n",
    "    . \"U.S. City Demographic Data\": This data comes from OpenSoft. \n",
    "    . \"Airport Code Table\": This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define the needed resources of spark\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "immigration_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "immigration_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading Global Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Global Temperature Data using Pandas Library\n",
    "imported_file = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_GlobalTemperature = pd.read_csv(imported_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                object\n",
       "AverageTemperature               float64\n",
       "AverageTemperatureUncertainty    float64\n",
       "City                              object\n",
       "Country                           object\n",
       "Latitude                          object\n",
       "Longitude                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing the data type of the dataset\n",
    "df_GlobalTemperature.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the DF Tuple\n",
    "df_GlobalTemperature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "5  1744-04-01               5.788                          3.624  Århus   \n",
       "6  1744-05-01              10.644                          1.283  Århus   \n",
       "7  1744-06-01              14.051                          1.347  Århus   \n",
       "8  1744-07-01              16.082                          1.396  Århus   \n",
       "9  1744-08-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  \n",
       "5  Denmark   57.05N    10.33E  \n",
       "6  Denmark   57.05N    10.33E  \n",
       "7  Denmark   57.05N    10.33E  \n",
       "8  Denmark   57.05N    10.33E  \n",
       "9  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking the first tenth Global Temperature Rows\n",
    "df_GlobalTemperature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading AIRPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the Airport data using pandas library\n",
    "provided_airport_data = 'airport-codes_csv.csv'\n",
    "df_airport_data = pd.read_csv(provided_airport_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the DF Tuple\n",
    "df_airport_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process of removing the rows containing the null values for the ['iata_code', 'local_code']\n",
      "Before cleaning up, the total rows consists of 55075 rows.\n",
      "After process of cleaning, the total rows include 2987 rows.\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the initial airport dataset\n",
    "list_col = ['iata_code', 'local_code']\n",
    "df_airport_data = main.remove_error_values(df_airport_data,list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>03N</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Utirik Airport</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OC</td>\n",
       "      <td>MH</td>\n",
       "      <td>MH-UTI</td>\n",
       "      <td>Utirik Island</td>\n",
       "      <td>K03N</td>\n",
       "      <td>UTK</td>\n",
       "      <td>03N</td>\n",
       "      <td>169.852005, 11.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>07FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>07FA</td>\n",
       "      <td>OCA</td>\n",
       "      <td>07FA</td>\n",
       "      <td>-80.274803161621, 25.325399398804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PQS</td>\n",
       "      <td>0AK</td>\n",
       "      <td>-162.899994, 61.934601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0CO2</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Crested Butte Airpark</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>Crested Butte</td>\n",
       "      <td>0CO2</td>\n",
       "      <td>CSE</td>\n",
       "      <td>0CO2</td>\n",
       "      <td>-106.928341, 38.851918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>0TE7</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>LBJ Ranch Airport</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>Johnson City</td>\n",
       "      <td>0TE7</td>\n",
       "      <td>JCY</td>\n",
       "      <td>0TE7</td>\n",
       "      <td>-98.62249755859999, 30.251800537100003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>13MA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Metropolitan Airport</td>\n",
       "      <td>418.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MA</td>\n",
       "      <td>Palmer</td>\n",
       "      <td>13MA</td>\n",
       "      <td>PMX</td>\n",
       "      <td>13MA</td>\n",
       "      <td>-72.31140136719999, 42.223300933800004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>13Z</td>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Loring Seaplane Base</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Loring</td>\n",
       "      <td>13Z</td>\n",
       "      <td>WLR</td>\n",
       "      <td>13Z</td>\n",
       "      <td>-131.636993408, 55.6012992859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>16A</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Nunapitchuk Airport</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Nunapitchuk</td>\n",
       "      <td>PPIT</td>\n",
       "      <td>NUP</td>\n",
       "      <td>16A</td>\n",
       "      <td>-162.440454, 60.905591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>16K</td>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Port Alice Seaplane Base</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Port Alice</td>\n",
       "      <td>16K</td>\n",
       "      <td>PTC</td>\n",
       "      <td>16K</td>\n",
       "      <td>-133.597, 55.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>19AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Icy Bay Airport</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Icy Bay</td>\n",
       "      <td>19AK</td>\n",
       "      <td>ICY</td>\n",
       "      <td>19AK</td>\n",
       "      <td>-141.662002563, 59.96900177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ident           type                      name  elevation_ft continent  \\\n",
       "223    03N  small_airport            Utirik Airport           4.0        OC   \n",
       "440   07FA  small_airport   Ocean Reef Club Airport           8.0       NaN   \n",
       "594    0AK  small_airport     Pilot Station Airport         305.0       NaN   \n",
       "673   0CO2  small_airport     Crested Butte Airpark        8980.0       NaN   \n",
       "1088  0TE7  small_airport         LBJ Ranch Airport        1515.0       NaN   \n",
       "1402  13MA  small_airport      Metropolitan Airport         418.0       NaN   \n",
       "1438   13Z  seaplane_base      Loring Seaplane Base           0.0       NaN   \n",
       "1555   16A  small_airport       Nunapitchuk Airport          12.0       NaN   \n",
       "1574   16K  seaplane_base  Port Alice Seaplane Base           0.0       NaN   \n",
       "1722  19AK  small_airport           Icy Bay Airport          50.0       NaN   \n",
       "\n",
       "     iso_country iso_region   municipality gps_code iata_code local_code  \\\n",
       "223           MH     MH-UTI  Utirik Island     K03N       UTK        03N   \n",
       "440           US      US-FL      Key Largo     07FA       OCA       07FA   \n",
       "594           US      US-AK  Pilot Station      NaN       PQS        0AK   \n",
       "673           US      US-CO  Crested Butte     0CO2       CSE       0CO2   \n",
       "1088          US      US-TX   Johnson City     0TE7       JCY       0TE7   \n",
       "1402          US      US-MA         Palmer     13MA       PMX       13MA   \n",
       "1438          US      US-AK         Loring      13Z       WLR        13Z   \n",
       "1555          US      US-AK    Nunapitchuk     PPIT       NUP        16A   \n",
       "1574          US      US-AK     Port Alice      16K       PTC        16K   \n",
       "1722          US      US-AK        Icy Bay     19AK       ICY       19AK   \n",
       "\n",
       "                                 coordinates  \n",
       "223                       169.852005, 11.222  \n",
       "440        -80.274803161621, 25.325399398804  \n",
       "594                   -162.899994, 61.934601  \n",
       "673                   -106.928341, 38.851918  \n",
       "1088  -98.62249755859999, 30.251800537100003  \n",
       "1402  -72.31140136719999, 42.223300933800004  \n",
       "1438           -131.636993408, 55.6012992859  \n",
       "1555                  -162.440454, 60.905591  \n",
       "1574                        -133.597, 55.803  \n",
       "1722             -141.662002563, 59.96900177  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking the first tenth Airport Data Rows\n",
    "df_airport_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading US CITY DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the US City Demographics data using pandas library\n",
    "#imported_demographic_data = \"us-cities-demographics.csv\"\n",
    "#df_demographics = pd.read_csv(imported_demographic_data, delimiter=';')\n",
    "us_cities_dem_csv = \"us-cities-demographics.csv\"\n",
    "df_demographics = pd.read_csv(us_cities_dem_csv, delimiter=';')\n",
    "# Print the first five data rows\n",
    "df_demographics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the DF Tuple\n",
    "df_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8          40601.0   \n",
       "1            Quincy   Massachusetts        41.0          44129.0   \n",
       "2            Hoover         Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga      California        34.5          88127.0   \n",
       "4            Newark      New Jersey        34.6         138040.0   \n",
       "5            Peoria        Illinois        33.1          56229.0   \n",
       "6          Avondale         Arizona        29.1          38712.0   \n",
       "7       West Covina      California        39.8          51629.0   \n",
       "8          O'Fallon        Missouri        36.0          41762.0   \n",
       "9        High Point  North Carolina        35.5          51751.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "5            62432.0            118661              6634.0        7517.0   \n",
       "6            41971.0             80683              4815.0        8355.0   \n",
       "7            56860.0            108489              3800.0       37038.0   \n",
       "8            43270.0             85032              5783.0        3269.0   \n",
       "9            58077.0            109828              5204.0       16315.0   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking the first tenth US City Demographics Rows\n",
    "df_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column Male Population has null value\n",
      "The column Female Population has null value\n",
      "The column Number of Veterans has null value\n",
      "The column Foreign-born has null value\n",
      "The column Average Household Size has null value\n",
      "Finish the process of checking for null data\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Checking after the process of data cleaning\n",
    "for column in df_demographics:\n",
    "    values = df_demographics[column].unique()\n",
    "    if(True in pd.isnull(values)):\n",
    "        print(f\"The column {column} has null value\")\n",
    "print(\"Finish the process of checking for null data\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Build country code to country name data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324</td>\n",
       "      <td>ANGOLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>529</td>\n",
       "      <td>ANGUILLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>518</td>\n",
       "      <td>ANTIGUA-BARBUDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>687</td>\n",
       "      <td>ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>151</td>\n",
       "      <td>ARMENIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>532</td>\n",
       "      <td>ARUBA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code          country\n",
       "0  236      AFGHANISTAN\n",
       "1  101          ALBANIA\n",
       "2  316          ALGERIA\n",
       "3  102          ANDORRA\n",
       "4  324           ANGOLA\n",
       "5  529         ANGUILLA\n",
       "6  518  ANTIGUA-BARBUDA\n",
       "7  687       ARGENTINA \n",
       "8  151          ARMENIA\n",
       "9  532            ARUBA"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split and extract the two columns namely \"Country Name\" and \"Country Code\" from the provided data source \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "with open(\"I94_SAS_Labels_Descriptions.SAS\") as f:\n",
    "    contents = f.readlines()\n",
    "    country_code = {}\n",
    "    for countries in contents[10:298]:\n",
    "        pair = countries.split('=')\n",
    "        code,country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "        country_code[code] = country\n",
    "df_country_code = pd.DataFrame(list(country_code.items()),columns=['code','country'])\n",
    "# Picking the first tenth country data and the corresponding code.\n",
    "df_country_code.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Pyspark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "##### Identify data quality issues, like missing values, duplicate data, etc.\n",
    "    . For the criteria like data quality issues that the method is just to make some creative ways to enhance the quality of the dataset such as add extra columns, rows if needed or even look for the more distinct suitable datasets on other data resources.\n",
    "    . For some case like missing value, duplicate data, de-dupe data,... The stategy is just to remove and limit the column or row happing errors like that. \n",
    "\n",
    "#### Cleaning Steps\n",
    "##### The necessary steps to clean the data including:\n",
    "    . Step 1: Remove duplicate or irrelevant observations. Remove unwanted observations from your dataset, including duplicate observations or irrelevant observations. \n",
    "    . Step 2: Fix structural errors. \n",
    "    . Step 3: Filter unwanted outliers. \n",
    "    . Step 4: Handle missing data. \n",
    "    . Step 5: Validate and make the QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks \n",
    "immigration_spark = immigration_spark.drop(\"insnum\",\"occup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up Immigration data\n",
    "immigration_spark = immigration_spark.where(immigration_spark.arrdate.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_spark = immigration_spark.where(immigration_spark.depdate.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up Global Temperature data\n",
    "global_temperature_df = global_temperature_df.dropna()\n",
    "# List out the some data rows for viewing\n",
    "global_temperature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#discard historical data and work with data for past 20 years\n",
    "print(\"Discarding historical data and work with data for recent 20 years\")\n",
    "dt_begin = \"2000-01-01\"\n",
    "dt_end = \"2019-01-01\"\n",
    "after_dt_begin = global_temperature_df[\"dt\"] >= dt_begin\n",
    "before_dt_end = global_temperature_df[\"dt\"] < dt_end\n",
    "dt_range = after_dt_begin & before_dt_end\n",
    "global_temperature_df = global_temperature_df.loc[dt_range]\n",
    "global_temperature_df.shape\n",
    "global_temperature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up Airport codes\n",
    "column_list = ['iata_code', 'local_code']\n",
    "airport_codes_df = main.cleanup_missing_column_values(airport_codes_df,column_list)\n",
    "# List out the some data rows for viewing\n",
    "airport_codes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "##### Utilizing the relational star schema for creating the database, and the core purpose of the DB foucuses on around the tables of immigrants with:\n",
    "    . The Fact table : applying a star schema for this context. The fact table does not follow the obvious structure that all dimension tables will connect directly to the fact table. \n",
    "    . The Dimension table : showing the main context for immigration patterns that can give the US goverments more better decisions. \n",
    "    And these tables are shaped with each other and then truncated after finish a process. Lately, the core goal of helping the US government and International citizens address for the US immigration problems effectively.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "##### List the steps necessary to pipeline the data into the chosen data model: \n",
    "    . Create the high-level data pipeline flow for ETL process.\n",
    "    . Design a detailed data pipeline utilizing the stength of AWS ecosystem.\n",
    "    . Make a corresopnding for each steps in the data pipeline when implementing the ETL process.\n",
    "    . Optimize the data pipeline flow as well as the ETL process for later usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "##### Build the data pipelines to create the data model.\n",
    "General architecture : Using the Apache Airflow for creating the data pipeline and utilizing the Apache Spark to pull the data and store them in a data lake on an S3 bucket in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define immigration data\n",
    "print(immigration_spark.count(),len(immigration_spark.columns))\n",
    "\n",
    "# Design the schema\n",
    "globaltemp_schema = StructType([StructField(\"dt\", StringType(), True)\\\n",
    "                          ,StructField(\"average_temperature\", FloatType(), True)\\\n",
    "                          ,StructField(\"average_temperature_uncertainty\", FloatType(), True)\\\n",
    "                          ,StructField(\"city\", StringType(), True)\\\n",
    "                          ,StructField(\"country\", StringType(), True)\\\n",
    "                          ,StructField(\"latitude\", StringType(), True)\\\n",
    "                          ,StructField(\"longitude\", StringType(), True)])\n",
    "global_temperature_df.rename(columns={'AverageTemperature':'average_temperature'}, inplace=True)\n",
    "global_temperature_df.rename(columns={'AverageTemperatureUncertainty':'average_temperature_uncertainty'}, inplace=True)\n",
    "global_temperature_df.rename(columns={'City':'city'}, inplace=True)\n",
    "global_temperature_df.rename(columns={'Country':'country'}, inplace=True)\n",
    "global_temperature_df.rename(columns={'Latitude':'latitude'}, inplace=True)\n",
    "global_temperature_df.rename(columns={'Longitude':'longitude'}, inplace=True)\n",
    "\n",
    "temp_spark = spark.createDataFrame(global_temperature_df, schema=globaltemp_schema)\n",
    "\n",
    "temp_spark.toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dem_schema = StructType([StructField(\"City\", StringType(), True)\\\n",
    "                        ,StructField(\"State\", StringType(), True)\\\n",
    "                        ,StructField(\"Median Age\", FloatType(), True)\\\n",
    "                        ,StructField(\"Male Population\", FloatType(), True)\\\n",
    "                        ,StructField(\"Female Population\", FloatType(), True)\\\n",
    "                        ,StructField(\"Total Population\", IntegerType(), True)\\\n",
    "                        ,StructField(\"Number of Veterans\", FloatType(), True)\\\n",
    "                        ,StructField(\"Foreign-born\", FloatType(), True)\\\n",
    "                        ,StructField(\"Average Household Size\", FloatType(), True)\\\n",
    "                        ,StructField(\"State Code\", StringType(), True)\\\n",
    "                        ,StructField(\"Race\", StringType(), True)\\\n",
    "                        ,StructField(\"Count\", IntegerType(), True)])\n",
    "\n",
    "dem_spark = spark.createDataFrame(demographics_df, schema=dem_schema)\n",
    "dem_spark.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the airport schema\n",
    "airport_schema =  StructType([StructField(\"ident\", StringType(), True)\\\n",
    "                        ,StructField(\"type\", StringType(), True)\\\n",
    "                        ,StructField(\"name\", StringType(), True)\\\n",
    "                        ,StructField(\"elevation_ft\", FloatType(), True)\\\n",
    "                        ,StructField(\"continent\", StringType(), True)\\\n",
    "                        ,StructField(\"iso_country\", StringType(), True)\\\n",
    "                        ,StructField(\"iso_region\", StringType(), True)\\\n",
    "                        ,StructField(\"municipality\", StringType(), True)\\\n",
    "                        ,StructField(\"gps_code\", StringType(), True)\\\n",
    "                        ,StructField(\"iata_code\", StringType(), True)\\\n",
    "                        ,StructField(\"local_code\", StringType(), True)\\\n",
    "                        ,StructField(\"coordinates\", StringType(), True)])\n",
    "airport_spark = spark.createDataFrame(airport_codes_df, schema=airport_schema)\n",
    "# Using pandas for viewing some data rows.\n",
    "airport_spark.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a path storing the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the directory of output\n",
    "output_directory=\"destination_table/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define airport dimension\n",
    "dim_airport = helper.create_airport(airport_spark, output_path)\n",
    "\n",
    "# Define status dimension\n",
    "dim_status = helper.create_status(immigration_spark,output_path)\n",
    "\n",
    "# Define status dimension\n",
    "dim_time = helper.create_time(immigration_spark,output_path)\n",
    "\n",
    "# Define visa dimension\n",
    "dim_visa = helper.create_visa(immigration_spark,output_path)\n",
    "\n",
    "# Define country dimension\n",
    "dim_country= spark.createDataFrame(country_code_df)\n",
    "country = helper.create_country(dim_country,output_path)\n",
    "\n",
    "# Define temperature dimension\n",
    "temperature = create_temperature(temp_spark,output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Immigration Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# join city dimension and temperature dimension\n",
    "country_temperature = country.select([\"*\"])\\\n",
    "            .join(temperature, (country.country == upper(temperature.country)), how='full')\\\n",
    "            .select([country.code, country.country, temperature.temperature_id, temperature.average_temperature, temperature.average_temperature_uncertainty])\n",
    "\n",
    "country_temperature.write.mode(\"overwrite\").parquet(output_path+\"country_temperature_mapping\")\n",
    "\n",
    "# Define the immigration data\n",
    "immigration = helper.create_immigration(immigration_spark, output_path, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing : create some realted unit tests for specific files in order to evaluate the data quality.  \n",
    " * Source/Count checks to ensure completeness : apply the Quality checks some situations like (duplicate, unique and no-null data) \n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a genneric data examining for airport data\n",
    "airport = spark.read.parquet(output_path+\"airport\")\n",
    "airport.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for status data\n",
    "status = spark.read.parquet(output_path+\"status\")\n",
    "status.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for time data\n",
    "time = spark.read.parquet(output_path+\"time\")\n",
    "time.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for visa data\n",
    "visa = spark.read.parquet(output_path+\"visa\")\n",
    "visa.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for temperature data\n",
    "temperature = spark.read.parquet(output_path+\"temperature\")\n",
    "temperature.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for country data\n",
    "country = spark.read.parquet(output_path+\"country\")\n",
    "country.toPandas().dtypes\n",
    "\n",
    "# Define a genneric data examining for immigration data\n",
    "immigration = spark.read.parquet(output_path+\"immigration\")\n",
    "immigration.toPandas().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks whether the records are completed for every destination tables defined above. \n",
    "main.run_record_count_check(airport, \"airport\")\n",
    "mani.run_record_count_check(time, \"time\")\n",
    "main.run_record_count_check(status, \"status\")\n",
    "main.run_record_count_check(visa, \"visa\")\n",
    "main.run_record_count_check(state, \"state\")\n",
    "main.run_record_count_check(temperature, \"temperature\")\n",
    "main.run_record_count_check(immigration, \"immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for the data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Using the \"Data Dictionary file\" for witnessing more about the case of data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "    . Take the advantages of AWS ecosystem with lots of useful and effective services for optimizing a vast of the implementing time and budget.\n",
    "    . Utilizing the technologies namley Apache Spark (Pyspark) AWS S3, EMR, EC2, and AWS Redshift.\n",
    "    . AWS EMR (with EC2) : just due to the fact that the spark dependencies come preloaded and it helps to prevent the action of manually set up the dependencies on each cluster and be easier to add more packages and fucntionality if needed.\n",
    "    . AWS S3 : it can bring the lucrative case for the goverment and restore more high-capacity resources. And using the S3 bucket can help me to save memory space and allocation via storing the processed files in parquet format.\n",
    "    . AWS Redshift : helps the related people to access the data resources when needed. Besides, using Redshift take a greater price of usage, and it can help in making the relevant settings and other configurations when implementing.\n",
    "    \n",
    "* ##### Propose how often the data should be updated and why:\n",
    "    . The data should be updated every quarters because the data quantity will be increased sharply via hours or even seconds with a great deal of one, so that it really needs to update the data with this assigned schedule.\n",
    "    \n",
    "* ##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * ##### The data was increased by 100x.\n",
    "     . Applying the AWS Redshift can handle big reads and writes so there wouldn't need to make some changes when allocating more nodes for the emr cluster to scale with the data quantity.\n",
    "     \n",
    " * ##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     . For solving this requirement, definitely needing to utilize AWS EventBrigde or maybe the Airflow and have a daily notification when implementing on 7am every day and then send the results via email about the status of the data process.\n",
    "     \n",
    " * ##### The database needed to be accessed by 100+ people.\n",
    "     . Utilizing the AWS redshift will handle effectively this senario with satisfying +100 people at the same time with the must-have condition that these people have to own the correct AWS credentials like IAM key and secret key, so that these people would be able to implement their own exploration and reports when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
